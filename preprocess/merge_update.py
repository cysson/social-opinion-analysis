# æ–‡ä»¶å: preprocess/merge_safe.py
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

# 1. å¯åŠ¨ Spark
# å¢åŠ ä¸€äº›é…ç½®ï¼Œå…è®¸è¯»å–æ¯”è¾ƒè„çš„ CSV
spark = SparkSession.builder \
    .appName("Safe_Merge_Data") \
    .master("local[*]") \
    .config("spark.sql.shuffle.partitions", "10") \
    .getOrCreate()
spark.sparkContext.setLogLevel("WARN")

print(">>> [1/4] å¯åŠ¨å®‰å…¨æ¨¡å¼è¯»å– (å¼ºåˆ¶æ‰€æœ‰åˆ—ä¸º String)...")

# å®šä¹‰è¯»å–å‡½æ•°ï¼šå¼ºåˆ¶ä¸æ¨æ–­ç±»å‹ (inferSchema=False)
# è¿™æ · '123' å’Œ 'https://...' éƒ½ä¼šè¢«å½“åšå­—ç¬¦ä¸²ï¼Œç»å¯¹ä¸ä¼šæŠ¥é”™
def read_csv_safe(path):
    try:
        # quote="\"" å’Œ escape="\"" å¤„ç†è¯„è®ºé‡Œå¸¦é€—å·æˆ–å¼•å·çš„æƒ…å†µ
        # multiLine=True å…è®¸ä¸€æ¡è¯„è®ºæ¢è¡Œ
        df = spark.read.option("header", "true") \
                       .option("inferSchema", "false") \
                       .option("multiLine", "true") \
                       .option("quote", "\"") \
                       .option("escape", "\"") \
                       .csv(path)
        print(f"âœ… æˆåŠŸè¯»å–: {path} | æ•°æ®é‡: {df.count()}")
        return df
    except Exception as e:
        print(f"âš ï¸ è¯»å–å¤±è´¥: {path}")
        # print(e) # è°ƒè¯•æ—¶å¯ä»¥æ‰“å¼€
        return None

# 2. è¯»å–æ–‡ä»¶
df_old = read_csv_safe("data/cleaned/old_data.csv")
df_new = read_csv_safe("data/cleaned/cleaned_comments.csv")

# 3. åˆå¹¶
print(">>> [2/4] æ­£åœ¨åˆå¹¶...")
if df_old is not None and df_new is not None:
    # è‡ªåŠ¨å¯¹é½åˆ—å
    df_merged = df_old.unionByName(df_new, allowMissingColumns=True)
elif df_old is not None:
    df_merged = df_old
elif df_new is not None:
    df_merged = df_new
else:
    print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶ï¼è¯·æ£€æŸ¥ data/cleaned/ ç›®å½•ä¸‹æ˜¯å¦æœ‰ old_data.csv å’Œ cleaned_comments.csv")
    exit()

print(f"   åˆå¹¶ååŸå§‹æ•°é‡: {df_merged.count()}")

# 4. å»é‡
print(">>> [3/4] æ­£åœ¨å»é‡...")
if "content" in df_merged.columns:
    df_final = df_merged.dropDuplicates(["content"])
else:
    df_final = df_merged.dropDuplicates()

print(f"âœ… å»é‡åæœ€ç»ˆæ•°é‡: {df_final.count()}")

# 5. ä¿å­˜
# ä¿å­˜æ—¶ä¹Ÿä¿ç•™ headerï¼Œä¸”ä¸åšä»»ä½•å‹ç¼©ï¼Œä¿è¯å…¼å®¹æ€§
output_path = "data/cleaned/final_all_data.csv"
print(f">>> [4/4] æ­£åœ¨ä¿å­˜åˆ° {output_path} ...")

df_final.coalesce(1).write \
    .option("header", "true") \
    .option("quote", "\"") \
    .option("escape", "\"") \
    .mode("overwrite") \
    .csv(output_path)

print("\nğŸ‰ åˆå¹¶æˆåŠŸï¼")
print("æ³¨æ„ï¼šå› ä¸ºæˆ‘ä»¬å¼ºåˆ¶ä½¿ç”¨äº† String ç±»å‹è¯»å–ï¼Œåç»­è¿è¡Œ analysis è„šæœ¬æ—¶ï¼Œ")
print("Spark ä¼šè‡ªåŠ¨å°è¯•å°† String è½¬å›æ•°å­—/æ—¥æœŸã€‚å¦‚æœ analysis æŠ¥é”™ï¼Œ")
print("è¯·æ£€æŸ¥ final_all_data.csv é‡Œæ˜¯å¦çœŸçš„æ··å…¥äº†å¥‡æ€ªçš„ URLã€‚")

spark.stop()