# æ–‡ä»¶å: visualization/analysis_sentiment_model.py
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, length, when
from pyspark.sql.types import FloatType, ArrayType, StringType, IntegerType
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import jieba
import pandas as pd
import os

# ================= 1. å‡†å¤‡å·¥ä½œ =================
# å®šä¹‰æƒ…æ„Ÿè¯å…¸ (ç”¨äºç”Ÿæˆè®­ç»ƒæ ‡ç­¾)
POSITIVE_WORDS = set(['å–œæ¬¢', 'ä¸é”™', 'æ”¯æŒ', 'åŠ æ²¹', 'å¸Œæœ›', 'æˆåŠŸ', 'ä¸Šå²¸', 'æœºä¼š', 'å‘å±•', 'ä¼˜ç§€', 'å¥½', 'æ£’', 'èµ', 'å¼€å¿ƒ', 'å¿«ä¹'])
NEGATIVE_WORDS = set(['ç„¦è™‘', 'æ‹…å¿ƒ', 'å®³æ€•', 'å¤±ä¸š', 'è£å‘˜', 'è¿·èŒ«', 'åƒåœ¾', 'æ¶å¿ƒ', 'è®¨åŒ', 'å·®', 'éš¾', 'ç´¯', 'å·', 'ç—›è‹¦', 'å¤±æœ›'])

# è®¡ç®—æƒ…æ„Ÿå¾—åˆ† (ç”¨äºåˆ†æ)
def get_score(text):
    if not text: return 0.0
    score = 0
    for w in POSITIVE_WORDS:
        if w in text: score += 1
    for w in NEGATIVE_WORDS:
        if w in text: score -= 1
    return float(score)

# åˆ†è¯å‡½æ•° (ç”¨äºæœºå™¨å­¦ä¹ )
def jieba_cut(text):
    if not text: return []
    return [w for w in jieba.cut(text) if len(w) > 1]

# ================= 2. å¯åŠ¨ Spark =================
spark = SparkSession.builder \
    .appName("Sentiment_NaiveBayes") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()
spark.sparkContext.setLogLevel("WARN")

print(">>> [1/4] è¯»å–æ•°æ®...")
df = spark.read.csv("data/cleaned/final_all_data.csv", header=True, inferSchema=True)
df = df.dropna(subset=["content"])

# æ³¨å†Œ UDF
score_udf = udf(get_score, FloatType())
jieba_udf = udf(jieba_cut, ArrayType(StringType()))

# ================= 3. ç‰¹å¾å·¥ç¨‹ (Feature Engineering) =================
print(">>> [2/4] æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾æ„å»º...")

# 1. è®¡ç®—é•¿åº¦ & æƒ…æ„Ÿå¾—åˆ†
df_processed = df.withColumn("content_len", length(col("content"))) \
                 .withColumn("score", score_udf(col("content"))) \
                 .withColumn("words", jieba_udf(col("content")))

# 2. ç”Ÿæˆæ ‡ç­¾ (Label) ç”¨äºè®­ç»ƒè´å¶æ–¯
# è§„åˆ™ï¼šåˆ† > 0 æ ‡ä¸º 1 (æ­£å‘)ï¼Œåˆ† < 0 æ ‡ä¸º 0 (è´Ÿå‘)ï¼Œ0åˆ†çš„ä¸å‚ä¸è®­ç»ƒ
df_labeled = df_processed.filter(col("score") != 0) \
    .withColumn("label", when(col("score") > 0, 1.0).otherwise(0.0))

# 3. TF-IDF å‘é‡åŒ– (æœºå™¨å­¦ä¹ çš„è¾“å…¥å¿…é¡»æ˜¯æ•°å­—å‘é‡)
# HashingTF å°†æ–‡æœ¬è½¬ä¸ºé¢‘ç‡å‘é‡
hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=2000)
featurizedData = hashingTF.transform(df_labeled)

# IDF è°ƒæ•´æƒé‡
idf = IDF(inputCol="rawFeatures", outputCol="features")
idfModel = idf.fit(featurizedData)
rescaledData = idfModel.transform(featurizedData)

# ================= 4. è®­ç»ƒæœ´ç´ è´å¶æ–¯æ¨¡å‹ (Naive Bayes) =================
print(">>> [3/4] è®­ç»ƒ Naive Bayes åˆ†ç±»å™¨...")

# æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
(trainingData, testData) = rescaledData.randomSplit([0.8, 0.2], seed=1234)

# è®­ç»ƒæ¨¡å‹
nb = NaiveBayes(smoothing=1.0, modelType="multinomial")
model = nb.fit(trainingData)

# é¢„æµ‹
predictions = model.transform(testData)

# è¯„ä¼°å‡†ç¡®ç‡
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print(f"\n==========================================")
print(f"ğŸ¤– æ¨¡å‹è¯„ä¼°ç»“æœ (Model Evaluation):")
print(f"   ç®—æ³•: Naive Bayes (æœ´ç´ è´å¶æ–¯)")
print(f"   å‡†ç¡®ç‡ (Accuracy): {accuracy:.2%}")
print(f"==========================================\n")

# ================= 5. å¯¼å‡ºæ•°æ®ç”¨äºç®±çº¿å›¾åˆ†æ =================
print(">>> [4/4] å¯¼å‡ºæ•°æ®ç”¨äºå¯è§†åŒ– (é•¿åº¦ vs æƒ…æ„Ÿ)...")

# ã€ä¿®æ”¹å¤„ã€‘è®¡ç®—å½’ä¸€åŒ–å¾—åˆ†ï¼š (åŸå§‹å¾—åˆ† / è¯„è®ºé•¿åº¦) * 100
# å«ä¹‰ï¼šæ¯ 100 ä¸ªå­—çš„æƒ…æ„Ÿå¼ºåº¦
export_df = df_processed.filter(col("content_len") > 0) \
    .withColumn("norm_score", (col("score") / col("content_len")) * 100) \
    .select("content_len", "norm_score") \
    .sample(fraction=0.5, seed=42) 

pdf_export = export_df.toPandas()

# ä¿å­˜
pdf_export.to_csv("visualization/result_length_sentiment.csv", index=False)
print("âœ… ç»“æœå·²ä¿å­˜: visualization/result_length_sentiment.csv")

spark.stop()