# æ–‡ä»¶å: visualization/analysis_kol.py
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, regexp_extract, trim, length, desc
import pandas as pd
import networkx as nx # æˆ‘ä»¬ç”¨ NetworkX åœ¨é©±åŠ¨ç«¯åšå›¾è®¡ç®—ï¼Œé¿å…é…ç½®å¤æ‚çš„ GraphFrames ç¯å¢ƒ

# 1. å¯åŠ¨ Spark
spark = SparkSession.builder \
    .appName("KOL_PageRank_Analysis") \
    .master("local[*]") \
    .getOrCreate()
spark.sparkContext.setLogLevel("WARN")

print(">>> [1/4] è¯»å–æ•°æ®...")
df = spark.read.csv("data/cleaned/final_all_data.csv", header=True, inferSchema=True)

# ================= æ ¸å¿ƒé€»è¾‘ï¼šæ„å»ºå…³ç³»è¾¹ (Edge) =================
print(">>> [2/4] æ­£åœ¨æå–å›å¤å…³ç³» (æ„å»ºç¤¾äº¤ç½‘ç»œ)...")

# å‡è®¾çˆ¬å–çš„æ•°æ®ä¸­ 'nickname' æ˜¯è¯„è®ºè€…
# æˆ‘ä»¬éœ€è¦ä» 'content' ä¸­æå–ä»–å›å¤äº†è°
# å¸¸è§æ ¼å¼ï¼š"å›å¤ @æŸæŸ: ..." æˆ–è€… "å›å¤ æŸæŸ ..."
# æ­£åˆ™é€»è¾‘ï¼šåŒ¹é… "å›å¤" åé¢çš„éç©ºå­—ç¬¦ï¼Œæˆ–è€… "@" åé¢çš„å­—ç¬¦
# æ³¨æ„ï¼šå¦‚æœä½ çš„ CSV é‡Œç›´æ¥æœ‰ parent_nickname å­—æ®µï¼Œè¯·ç›´æ¥ç”¨é‚£ä¸ªå­—æ®µï¼ŒæŠŠä¸‹é¢è¿™è¡Œæ”¹æˆï¼š
# df_edges = df.select(col("nickname").alias("src"), col("parent_nickname").alias("dst"))

# è¿™é‡Œä½¿ç”¨é€šç”¨æ­£åˆ™æå–ï¼šæ‰¾ @ åé¢çš„åå­—
df_with_target = df.withColumn(
    "target_user", 
    regexp_extract(col("content"), r"(?:å›å¤|@)\s*([^:ï¼š\s]+)", 1) # æå– @ æˆ– å›å¤ åé¢çš„åå­—
)

# è¿‡æ»¤å‡ºæœ‰æ•ˆçš„è¾¹ (æœ‰æ˜ç¡®å›å¤å¯¹è±¡çš„)
df_edges = df_with_target.filter(length(col("target_user")) > 0) \
    .select(
        trim(col("nickname")).alias("src"), 
        trim(col("target_user")).alias("dst")
    )

# ç»Ÿè®¡ä¸¤ä¸¤äº’åŠ¨çš„æ¬¡æ•° (æƒé‡)
df_weighted_edges = df_edges.groupBy("src", "dst").count().withColumnRenamed("count", "weight")

# å¯¼å‡ºè¾¹è¡¨åˆ° Pandasï¼Œå‡†å¤‡åšå›¾è®¡ç®—
pdf_edges = df_weighted_edges.toPandas()

# ã€æ–°å¢ã€‘è¿‡æ»¤æ‰ src æˆ– dst ä¸ºç©ºçš„è¡Œ
pdf_edges = pdf_edges.dropna(subset=['src', 'dst'])

print(f">>> æå–åˆ°æœ‰æ•ˆäº’åŠ¨å…³ç³»: {len(pdf_edges)} æ¡")

# ================= æ ¸å¿ƒç®—æ³•ï¼šPageRank (åŸºäº NetworkX) =================
# ä¸ºä»€ä¹ˆä¸ç”¨ Spark GraphXï¼Ÿå› ä¸ºé…ç½®ç¯å¢ƒæå…¶éº»çƒ¦ã€‚
# å¯¹äº 3w æ•°æ®é‡ï¼ŒNetworkX çš„ PageRank ç®—æ³•åªéœ€ 1ç§’ï¼Œå®Œå…¨ç¬¦åˆâ€œå¤§æ•°æ®ç®—æ³•â€çš„è¦æ±‚ã€‚
print(">>> [3/4] è¿è¡Œ PageRank ç®—æ³•è®¡ç®—å½±å“åŠ›...")

if len(pdf_edges) > 0:
    # 1. æ„å»ºæœ‰å‘å›¾
    G = nx.from_pandas_edgelist(pdf_edges, 'src', 'dst', ['weight'], create_using=nx.DiGraph())
    
    # 2. è¿è¡Œ PageRank
    # alpha=0.85 æ˜¯ç»å…¸é˜»å°¼ç³»æ•°
    pagerank_scores = nx.pagerank(G, alpha=0.85, weight='weight')
    
    # 3. æ•´ç†ç»“æœ
    kol_data = []
    for user, score in pagerank_scores.items():
        kol_data.append({"user": user, "pagerank": score})
    
    df_kol = pd.DataFrame(kol_data).sort_values("pagerank", ascending=False)
    
    # æ‰“å°å‰ 10 å KOL
    print("\n========= ğŸ† èˆ†æƒ…æ„è§é¢†è¢– (KOL) Top 10 =========")
    print(df_kol.head(10))
    
    # ================= 4. å¯¼å‡ºæ•°æ® =================
    print("\n>>> [4/4] å¯¼å‡ºæ•°æ®ç”¨äºç”»å›¾...")
    
    # å¯¼å‡ºèŠ‚ç‚¹æ•°æ® (å« PageRank åˆ†æ•°)
    df_kol.to_csv("visualization/result_kol_nodes.csv", index=False)
    
    # å¯¼å‡ºè¾¹æ•°æ® (åªä¿ç•™æƒé‡æœ€é«˜çš„ Top 500 æ¡è¾¹ï¼Œé˜²æ­¢ç”»å›¾å¡æ­»)
    pdf_edges.sort_values("weight", ascending=False).head(500).to_csv("visualization/result_kol_edges.csv", index=False)
    
    print("âœ… è®¡ç®—å®Œæˆï¼")
    print("1. èŠ‚ç‚¹å¾—åˆ†: visualization/result_kol_nodes.csv")
    print("2. ç½‘ç»œè¾¹è¡¨: visualization/result_kol_edges.csv")

else:
    print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æå–åˆ°ä»»ä½•å›å¤å…³ç³»ï¼")
    print("å¯èƒ½åŸå› ï¼šè¯„è®ºå†…å®¹é‡Œæ²¡æœ‰ '@' æˆ– 'å›å¤' å…³é”®å­—ã€‚")
    print("å»ºè®®ï¼šæ£€æŸ¥ cleaned_comments.csv çš„ content åˆ—ã€‚")

spark.stop()