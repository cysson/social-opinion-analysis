# 数据清洗代码改进说明

## 一、改进概述

根据工作要求和后续分析建议，对数据清洗代码进行了全面改进，使其更好地支持后续的大数据分析任务，包括：
- 情感分析算法（BERT/词典）
- 聚类算法（K-Means）
- 关联规则挖掘（FP-Growth）
- 5张可视化图表
- 3个论点论证

## 二、主要改进内容

### 1. 改进清洗步骤说明（`clean_data`方法）

**改进前**：清洗步骤说明简单，缺少原因说明

**改进后**：
- 为每个清洗步骤添加了详细说明和原因
- 明确标注清洗目标：提高数据质量，为后续分析做准备
- 每个步骤都有清晰的日志输出，便于追踪

**关键改进点**：
```python
# 每个步骤都包含：
# - 步骤名称
# - 原因说明
# - 结果统计
```

### 2. 增强缺失值处理（`handle_missing_values`方法）

**改进前**：简单删除或填充缺失值

**改进后**：
- **关键字段严格处理**：`content`、`user_id`、`create_time`缺失则删除记录
  - 原因：这些字段对后续分析至关重要
- **时间字段智能推断**：尝试从`last_modify_ts`推断`create_time`
  - 原因：时间对趋势分析至关重要
- **分析字段合理填充**：
  - `ip_location` → 填充为"未知"（用于地域分布图）
  - `parent_comment_id` → 填充为0（0表示顶级评论，用于网络图）
  - 数值字段 → 填充为0

**支持的分析**：
- ✅ 时间趋势图（确保时间字段完整）
- ✅ 地域分布图（地理位置字段完整）
- ✅ 互动关系网络图（parent_comment_id完整）

### 3. 优化文本清洗（`clean_text`方法）

**改进前**：移除所有特殊字符，可能丢失有用信息

**改进后**：
- 保留#话题标签（可用于话题分析）
- 移除URL、HTML标签、控制字符
- 规范化空格，保留中文、英文、数字、常用标点
- 保留更多有用文本信息

**支持的分析**：
- ✅ 情感分析（保留情感词汇）
- ✅ 聚类分析（保留主题关键词）
- ✅ 词云生成（保留高频词）

### 4. 扩展特征工程（`feature_engineering`方法）

**新增特征**：

#### 4.1 时间特征（支持时间趋势图）
- `year`、`quarter`、`month`、`day`、`hour`、`day_of_week`、`date`、`datetime_str`
- 用途：支持多维度的时间趋势分析

#### 4.2 地理位置特征（支持地域分布图）
- `location_standard`：国家/地区级别（中国、美国等）
- `location_province`：省份级别（北京、上海、广东等）
- `location_city`：城市级别（北京、上海、广州等）
- 用途：支持宏观和细粒度的地域分布分析

#### 4.3 用户活跃度特征（支持KOL识别）
- `user_comment_count`：用户评论数
- `user_total_likes`：用户总点赞数
- `user_total_replies`：用户总回复数
- `user_activity_score`：用户活跃度得分
- `is_kol`：是否为关键意见领袖（前10%活跃用户）
- 用途：识别KOL，支持"关键节点"传播理论论证

#### 4.4 情感特征（支持情感分析）
- `sentiment_score`：情感得分（-1到1）
- `sentiment_label`：情感标签（正面/中性/负面）
- 用途：支持情感分析和情感分布图

#### 4.5 互动特征（支持关联规则挖掘）
- `interaction_count`：总互动数（点赞+回复）
- `has_interaction`：是否有互动
- 用途：支持FP-Growth关联规则挖掘

### 5. 优化地理位置标准化

**新增方法**：
- `extract_province`：提取省份信息（支持34个省级行政区）
- `extract_city`：提取城市信息（支持主要城市）

**改进**：
- 支持更细粒度的地域分析
- 识别更多国家和地区
- 支持中英文混合识别

### 6. 增强情感分析

**改进前**：简单的关键词匹配

**改进后**：
- 扩展正面/负面词汇库（各30+个词汇）
- 更准确的情感得分计算
- 自动生成情感标签（正面/中性/负面）

**注意**：当前为基础版本，后续可使用BERT等深度学习模型提升准确性

### 7. 改进清洗报告（`generate_cleaning_report`方法）

**新增内容**：
- 清洗目标说明
- 7个清洗步骤的详细说明和原因
- 时间覆盖范围统计
- 地域分布统计
- 情感分布统计
- KOL数量统计
- 技术工具说明
- 输出格式说明

**用途**：直接用于报告中的"数据清洗与预处理"章节

## 三、支持后续分析的具体体现

### 1. 支持情感分析算法

**数据准备**：
- ✅ `content_clean`：清洗后的文本（保留情感词汇）
- ✅ `sentiment_score`：基础情感得分
- ✅ `sentiment_label`：情感分类标签

**后续可做**：
- 使用BERT模型进行更精确的情感分析
- 生成情感趋势可视化图
- 计算情感得分用于论点一论证

### 2. 支持聚类算法（K-Means）

**数据准备**：
- ✅ `content_clean`：清洗后的文本（保留主题关键词）
- ✅ `content_length`：文本长度特征
- ✅ `sentiment_score`：情感特征

**后续可做**：
- 对评论内容进行主题聚类
- 识别主要讨论话题
- 支持论点二论证

### 3. 支持关联规则挖掘（FP-Growth）

**数据准备**：
- ✅ `user_id`：用户ID（完整）
- ✅ `parent_comment_id`：回复关系（完整）
- ✅ `interaction_count`：互动数
- ✅ `is_reply`：是否为回复
- ✅ `is_kol`：是否为KOL

**后续可做**：
- 分析高频共现词
- 分析用户行为模式
- 识别KOL互动关系
- 支持论点三论证

### 4. 支持5张可视化图表

#### 4.1 评论数量随时间变化趋势图（折线图）
- ✅ `create_time`：时间字段（完整）
- ✅ `year`、`quarter`、`month`、`day`、`hour`：多维度时间特征
- ✅ `date`：日期字段

#### 4.2 情感极性分布图（饼图/柱状图）
- ✅ `sentiment_label`：情感标签（正面/中性/负面）
- ✅ `sentiment_score`：情感得分

#### 4.3 高频词云图
- ✅ `content_clean`：清洗后的文本（保留关键词）

#### 4.4 用户地域分布图（地图）
- ✅ `location_standard`：国家/地区级别
- ✅ `location_province`：省份级别
- ✅ `location_city`：城市级别

#### 4.5 评论互动关系图（网络图）
- ✅ `parent_comment_id`：父评论ID（完整）
- ✅ `user_id`：用户ID（完整）
- ✅ `is_reply`：是否为回复
- ✅ `is_kol`：KOL标识

#### 4.6 补充可视化图表建议

**4.6.1 时间-情感热力图**
- 数据字段：`date`、`hour`、`sentiment_label`
- 图表类型：热力图（Heatmap）
- 用途：展示不同时间段的情感分布变化，识别舆情爆发时段
- 优势：直观展示时间与情感的关联关系

**4.6.2 用户活跃度分布图**
- 数据字段：`user_activity_score`、`user_comment_count`
- 图表类型：直方图/箱线图
- 用途：展示用户活跃度的分布情况，识别长尾分布特征
- 优势：验证"二八定律"或幂律分布，支撑KOL识别

**4.6.3 评论长度分布图**
- 数据字段：`content_length`
- 图表类型：直方图/密度图
- 用途：分析评论长度分布，识别短评、长评比例
- 优势：了解用户表达习惯，为文本分析提供参考

**4.6.4 高频词条形图（Top N）**
- 数据字段：`content_clean`（分词后）
- 图表类型：水平条形图
- 用途：展示Top 20-30高频词，补充词云图的精确数值
- 优势：提供具体数值，便于量化分析

**4.6.5 地域-情感交叉分析图**
- 数据字段：`location_province`、`sentiment_label`
- 图表类型：分组柱状图/堆叠柱状图
- 用途：分析不同地域的情感倾向差异
- 优势：发现地域文化差异对话题态度的影响

**4.6.6 用户互动网络图（力导向图）**
- 数据字段：`user_id`、`parent_comment_id`、`is_kol`
- 图表类型：力导向网络图（Force-directed Graph）
- 用途：可视化用户之间的互动关系，突出KOL节点
- 优势：直观展示社交网络结构，识别核心节点和社区

**4.6.7 时间序列情感趋势图（多线图）**
- 数据字段：`date`、`sentiment_score`（按日聚合）
- 图表类型：多线图（正面/中性/负面三条线）
- 用途：展示不同情感类型随时间的变化趋势
- 优势：识别情感转折点，分析舆情演变过程

**4.6.8 KOL影响力对比图**
- 数据字段：`is_kol`、`like_count`、`sub_comment_count`
- 图表类型：分组柱状图/雷达图
- 用途：对比KOL与普通用户的互动数据差异
- 优势：量化KOL影响力，支撑论点三

**4.6.9 评论时段分布图**
- 数据字段：`hour`、`day_of_week`
- 图表类型：雷达图/热力图
- 用途：分析用户活跃时段，识别高峰时段
- 优势：了解用户行为模式，为内容发布策略提供参考

**4.6.10 聚类结果可视化图**
- 数据字段：聚类后的`cluster_id`、`sentiment_score`、`content_length`
- 图表类型：散点图（2D/3D）+ 颜色编码
- 用途：可视化K-Means聚类结果，展示主题分布
- 优势：直观展示评论的主题聚类效果

### 5. 支持3个论点论证

#### 论点一：AI职业规划话题的舆情分析
- ✅ `create_time`：时间趋势分析
- ✅ `sentiment_score`：情感分析
- ✅ `location_standard`：地域分布分析

#### 论点二：公众对"文科生进AI公司"的关切点
- ✅ `content_clean`：主题聚类分析
- ✅ `content_clean`：高频词分析

#### 论点三：KOL在话题传播中的枢纽作用
- ✅ `is_kol`：KOL识别
- ✅ `parent_comment_id`：用户互动关系
- ✅ `user_activity_score`：用户活跃度

#### 5.4 补充论点建议

**论点四：数据清洗质量对分析结果的影响**
- 数据支撑：
  - 清洗前后数据量对比（`cleaning_report.json`）
  - 关键字段完整性统计
  - 数据保留率分析
- 论证角度：
  - 说明数据清洗的必要性和重要性
  - 展示清洗后数据质量的提升
  - 验证清洗步骤的有效性
- 可视化支持：
  - 清洗前后数据量对比柱状图
  - 字段完整性雷达图
  - 数据质量指标仪表盘

**论点五：用户行为模式的地域差异分析**
- 数据支撑：
  - `location_province`：省份分布
  - `location_city`：城市分布
  - `sentiment_score`：情感得分（按地域聚合）
  - `content_length`：评论长度（按地域聚合）
- 论证角度：
  - 分析不同地域用户对话题的态度差异
  - 识别地域文化对话题讨论的影响
  - 发现地域特色观点
- 可视化支持：
  - 地域-情感交叉分析图
  - 地域分布热力图
  - 地域评论长度对比图

**论点六：话题传播的时间演化规律**
- 数据支撑：
  - `create_time`：时间序列
  - `sentiment_score`：情感演变
  - `interaction_count`：互动数变化
  - `hour`、`day_of_week`：时段分析
- 论证角度：
  - 识别话题传播的生命周期（萌芽期、爆发期、衰退期）
  - 分析情感演变的时间规律
  - 发现传播高峰时段
- 可视化支持：
  - 时间序列情感趋势图
  - 评论数量时间趋势图
  - 时间-情感热力图

**论点七：短评与长评的情感表达差异**
- 数据支撑：
  - `content_length`：评论长度
  - `sentiment_score`：情感得分
  - `sentiment_label`：情感标签
- 论证角度：
  - 分析不同长度评论的情感倾向差异
  - 验证"短评更极端，长评更理性"的假设
  - 识别表达方式对情感强度的影响
- 可视化支持：
  - 评论长度分布图
  - 长度-情感散点图
  - 长度分组情感对比图

**论点八：互动行为与内容质量的关系**
- 数据支撑：
  - `like_count`：点赞数
  - `sub_comment_count`：回复数
  - `content_length`：内容长度
  - `sentiment_score`：情感得分
- 论证角度：
  - 分析高互动内容的特点（长度、情感、主题）
  - 识别影响互动率的关键因素
  - 验证"优质内容获得更多互动"的假设
- 可视化支持：
  - 互动数-内容长度散点图
  - 互动数分布图
  - 高互动内容特征分析图

**论点九：话题讨论的社区结构特征**
- 数据支撑：
  - `parent_comment_id`：回复关系
  - `user_id`：用户ID
  - `is_kol`：KOL标识
  - 聚类结果：`cluster_id`
- 论证角度：
  - 分析话题讨论形成的社区结构
  - 识别不同观点社区（通过聚类）
  - 验证"观点极化"或"社区分化"现象
- 可视化支持：
  - 用户互动网络图
  - 聚类结果可视化图
  - 社区结构分析图

**论点十：数据驱动的舆情预警机制**
- 数据支撑：
  - 时间序列数据：`create_time`
  - 情感数据：`sentiment_score`、`sentiment_label`
  - 互动数据：`interaction_count`
  - 地域数据：`location_province`
- 论证角度：
  - 基于清洗后的数据构建舆情预警模型
  - 识别舆情爆发的早期信号
  - 验证数据清洗对预警准确性的影响
- 可视化支持：
  - 舆情预警指标仪表盘
  - 预警信号时间序列图
  - 预警准确性评估图

## 四、清洗步骤总结

| 步骤 | 名称 | 原因 | 支持的分析 |
|------|------|------|-----------|
| 1 | 删除完全空白的行 | 空白行无分析价值 | 所有分析 |
| 2 | 数据类型标准化 | 确保类型正确 | 所有分析 |
| 3 | 处理缺失值 | 关键字段完整 | 时间趋势、地域分布、网络图 |
| 4 | 文本内容清洗 | 保留有用文本 | 情感分析、聚类、词云 |
| 5 | 去重处理 | 避免重复影响统计 | 所有分析 |
| 6 | 特征工程 | 添加分析所需特征 | 所有分析 |
| 7 | 过滤噪音数据 | 提高数据质量 | 所有分析 |

## 五、技术工具说明

- **数据清洗工具**：Python Pandas
- **特征工程工具**：Python Pandas
- **输出格式**：
  - CSV：通用格式
  - Parquet：适合Hadoop/Spark分析
  - JSON Lines：适合流式处理

## 六、数据质量保证

### 清洗前后对比
- 原始数据：21,517条
- 清洗后数据：约14,000-15,000条（预计保留率65-70%）
- 数据保留率：合理范围内（移除无效数据，保留高质量数据）

### 关键字段完整性
- ✅ `content`：100%完整（删除缺失记录）
- ✅ `user_id`：100%完整（删除缺失记录）
- ✅ `create_time`：100%完整（删除缺失记录或从其他字段推断）
- ✅ `ip_location`：完整（缺失填充为"未知"）
- ✅ `parent_comment_id`：完整（缺失填充为0）

## 七、使用建议

1. **运行清洗脚本**：
   ```bash
   python src/data-cleaner.py
   ```

2. **查看清洗报告**：
   - 位置：`data/cleaned/cleaning_report.json`
   - 包含详细的清洗步骤和数据质量指标

3. **使用清洗后的数据**：
   - CSV格式：`data/cleaned/cleaned_comments.csv`
   - Parquet格式：`data/cleaned/cleaned_comments.parquet`（用于Spark）
   - JSON Lines格式：`data/cleaned/cleaned_comments.jsonl`

4. **报告撰写**：
   - 清洗报告中的内容可直接用于Word报告
   - 明确标注"本部分由[姓名]负责实施与验证"

## 八、后续分析建议

基于清洗后的数据，可以：

1. **使用Spark进行大规模分析**：
   - 读取Parquet格式数据
   - 运行情感分析、聚类、关联规则挖掘算法

2. **生成可视化图表**：
   - 使用清洗后的特征字段
   - 生成5张基础可视化图表（必需）
   - 根据数据特点选择补充图表（建议）
   - 推荐组合：基础5张 + 补充3-5张 = 8-10张图表

3. **论证3个论点**：
   - 使用清洗后的数据和特征
   - 进行统计分析，支撑论点
   - 基础3个论点（必需）
   - 根据数据特点选择补充论点（建议）
   - 推荐组合：基础3个 + 补充1-2个 = 4-5个论点

4. **图表与论点搭配建议**：
   - 每个论点至少配1-2张可视化图表
   - 图表类型多样化（折线图、柱状图、饼图、热力图、网络图等）
   - 确保图表能够直接支撑论点论证

## 九、改进总结

本次改进使数据清洗代码：
- ✅ 更好地支持后续分析需求
- ✅ 提供更详细的清洗步骤说明
- ✅ 添加更多有用的特征
- ✅ 确保关键字段完整性
- ✅ 生成更详细的清洗报告

所有改进都围绕"为后续分析做准备"这一核心目标，确保清洗后的数据能够直接支撑报告中的可视化图表、算法应用和论点论证。

## 十、可视化图表与论点搭配建议

### 10.1 基础组合（满足最低要求）

**5张基础图表**：
1. 评论数量随时间变化趋势图（折线图）
2. 情感极性分布图（饼图/柱状图）
3. 高频词云图
4. 用户地域分布图（地图）
5. 评论互动关系图（网络图）

**3个基础论点**：
1. AI职业规划话题的舆情分析（时间+情感+地域）
2. 公众对"文科生进AI公司"的关切点（聚类+高频词）
3. KOL在话题传播中的枢纽作用（网络分析+KOL识别）

### 10.2 增强组合（提升报告质量）

**推荐补充图表（选择3-5张）**：
- 时间-情感热力图（补充图表1）
- 用户活跃度分布图（补充图表2）
- 高频词条形图（补充图表3）
- 地域-情感交叉分析图（补充图表4）
- 时间序列情感趋势图（补充图表5）

**推荐补充论点（选择1-2个）**：
- 话题传播的时间演化规律（论点六）
- 用户行为模式的地域差异分析（论点五）
- 互动行为与内容质量的关系（论点八）

### 10.3 完整组合（追求卓越）

**10-12张图表**：
- 基础5张 + 补充5-7张
- 确保图表类型多样化
- 每个论点配2-3张图表

**5-6个论点**：
- 基础3个 + 补充2-3个
- 确保论点之间有逻辑关联
- 形成完整的论证体系

### 10.4 图表与论点对应关系

| 论点 | 支撑图表 | 数据字段 |
|------|---------|---------|
| 论点一：舆情分析 | 时间趋势图、情感分布图、地域分布图 | `create_time`、`sentiment_label`、`location_province` |
| 论点二：关切点分析 | 词云图、高频词条形图、聚类结果图 | `content_clean`、`cluster_id` |
| 论点三：KOL作用 | 互动网络图、KOL影响力对比图 | `is_kol`、`parent_comment_id`、`user_activity_score` |
| 论点四：清洗质量 | 清洗前后对比图、质量指标图 | `cleaning_report.json` |
| 论点五：地域差异 | 地域-情感交叉图、地域分布热力图 | `location_province`、`sentiment_score` |
| 论点六：时间演化 | 时间序列情感趋势图、时间-情感热力图 | `create_time`、`sentiment_score` |
| 论点七：长短评差异 | 评论长度分布图、长度-情感散点图 | `content_length`、`sentiment_score` |
| 论点八：互动关系 | 互动数分布图、互动-内容质量散点图 | `interaction_count`、`content_length` |
| 论点九：社区结构 | 用户互动网络图、聚类结果可视化图 | `parent_comment_id`、`cluster_id` |
| 论点十：预警机制 | 预警指标仪表盘、预警信号时间序列图 | `create_time`、`sentiment_score`、`interaction_count` |

### 10.5 实施建议

1. **优先完成基础组合**：确保满足最低要求
2. **根据数据特点选择补充内容**：如果地域数据丰富，优先选择地域相关图表和论点
3. **确保图表质量**：每张图表都要有清晰的标题、图例、数据标签
4. **论点论证要深入**：不仅要有数据支撑，还要有理论解释
5. **图表与论点要呼应**：每张图表都要服务于某个论点的论证

